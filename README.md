# AI-based Autonomic Management of Structured Parallel Programs

**Reinforcement Learning-driven Autoscaling for OpenFaaS Serverless Workflows**

This project implements an **RL-based autoscaling system** for serverless parallel task processing using OpenFaaS, Kubernetes, and Redis. It provides a Gym-compatible environment for training and evaluating autoscaling policies.

---

## ğŸ¯ Project Overview

### **What It Does**

- **Serverless Task Processing**: Producer-consumer workflow with OpenFaaS functions and calibrated image workloads
- **Dynamic Autoscaling**: RL agents learn optimal scaling policies (1-32 workers) with single-step adjustments informed by config-driven workload phases
- **QoS-Aware**: Tracks deadline compliance and optimizes for QoS targets
- **Baseline Comparison**: Reactive policies for benchmarking RL performance

### **Key Components**

1. **OpenFaaS Functions**: Emitter, Worker (scalable), Collector (gamma-sampled image workloads)
2. **Autoscaling Environment**: Gym-compatible RL environment
3. **RL Agents**: SARSA, lightweight DQN
4. **Baseline Policies**: ReactiveAverage, ReactiveMaximum
5. **Orchestrator**: Workflow management and monitoring

---

## ğŸ“ Project Structure

```
AI-based-Autonomic-Management-of-Structured-Parallel-Programs/
â”œâ”€â”€ autoscaling_env/                  # RL environment, baselines, comparison tooling
â”‚   â”œâ”€â”€ compare_policies.py           # Single-episode SARSA vs reactive comparison script
â”‚   â”œâ”€â”€ openfaas_autoscaling_env.py   # Gym-compatible OpenFaaS autoscaling environment
â”‚   â”œâ”€â”€ baselines/                    # Reactive baseline policies & docs
â”‚   â”‚   â”œâ”€â”€ reactive_policies.py
â”‚   â”‚   â”œâ”€â”€ test_reactive_baselines.py
â”‚   â”‚   â”œâ”€â”€ logs/                     # Baseline evaluation logs (gitignored)
â”‚   â”‚   â”œâ”€â”€ plots/                    # Baseline plots (gitignored)
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ rl/                           # RL agents (SARSA + lightweight DQN) & tooling
â”‚   â”‚   â”œâ”€â”€ sarsa_agent.py            # Tabular SARSA agent
â”‚   â”‚   â”œâ”€â”€ dqn_agent.py              # Compact neural DQN agent
â”‚   â”‚   â”œâ”€â”€ train_sarsa.py            # SARSA training CLI with per-step logs
â”‚   â”‚   â”œâ”€â”€ train_dqn.py              # DQN training CLI mirroring SARSA workflow
â”‚   â”‚   â”œâ”€â”€ test_sarsa.py             # SARSA evaluation & plotting
â”‚   â”‚   â”œâ”€â”€ test_dqn.py               # DQN evaluation & plotting
â”‚   â”‚   â”œâ”€â”€ plot_training.py          # Training curve renderer
â”‚   â”‚   â”œâ”€â”€ utils.py                  # Shared helpers (logging, directories)
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ runs/                         # Timestamped evaluation/comparison outputs (gitignored)
â”‚       â””â”€â”€ comparison/
â”œâ”€â”€ image_processing/                 # Workload calibration & task generation utilities
â”‚   â”œâ”€â”€ task_generator.py             # Phase-based arrival generator
â”‚   â”œâ”€â”€ calibrate_direct.py           # Image processing calibration pipeline
â”‚   â”œâ”€â”€ calibration_results.json
â”‚   â”œâ”€â”€ calibration_plot.png
â”‚   â”œâ”€â”€ task_arrival_rates.png
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ openfaas-functions/               # Deployed OpenFaaS functions
â”‚   â”œâ”€â”€ emitter/
â”‚   â”œâ”€â”€ worker/
â”‚   â”œâ”€â”€ collector/
â”‚   â”œâ”€â”€ template/                     # Custom python3-http-skeleton template
â”‚   â””â”€â”€ stack.yaml
â”œâ”€â”€ openfaas/                         # OpenFaaS deployment manifests
â”œâ”€â”€ utilities/                        # Shared orchestration helpers
â”‚   â”œâ”€â”€ configuration.yml             # System/experiment configuration
â”‚   â””â”€â”€ utilities.py
â”œâ”€â”€ plots/                            # Repository-level figures
â”œâ”€â”€ kind/                             # KIND cluster helpers
â”œâ”€â”€ cleanup.sh                        # Utility script for tearing down resources
â””â”€â”€ README.md                         # You are here
```

---

## ğŸ—ï¸ Architecture

### **Workflow Pattern**

```
  input_queue â†’ EMITTER â†’ worker_queue â†’ WORKER (Nx) â†’ result_queue â†’ COLLECTOR â†’ output_queue
                                       â†‘            â†“
                                       â””â”€ RL Agent â”€â”˜
                                        (Scale 1-32)

RL Agent observes: [input_q, worker_q, result_q, output_q, workers,
                    avg_time, max_time, arrival_rate, qos_rate]
```

### **RL Training Loop**

```
1. Observe: 9D state [input_q, worker_q, result_q, output_q, workers,
                      avg_time, max_time, arrival_rate, qos_rate]
2. Decide: RL agent (tabular SARSA or lightweight DQN) selects action {-1, 0, +1}
           via epsilon-greedy exploration
3. Act: Scale workers up/down via Kubernetes API
4. Reward: Based on QoS, queue length, worker cost, scaling penalty
5. Learn:
   - **SARSA**: On-policy TD update of the discretised Q-table (optionally with eligibility traces)
   - **DQN**: Store transition, sample mini-batches from replay, and apply Double-DQN updates to the policy network
```

---

## ğŸš€ Quick Start

### **Prerequisites**

- Kubernetes cluster with OpenFaaS installed
- Redis deployed in cluster
- `faas-cli`, `kubectl`, Python 3.13+
- Port forwards: `8080` (OpenFaaS), `6379` (Redis)

### **1. Deploy OpenFaaS Functions**

```bash
cd openfaas-functions
faas-cli up -f stack.yaml
```

### **2. Test Baseline Policies**

```bash
cd autoscaling_env/baselines
python test_reactive_baselines.py --agent both --steps 50 --step-duration 10 --horizon 10
```

### **3. Train SARSA Agent (tabular)**

```bash
cd autoscaling_env/rl
python train_sarsa.py --episodes 100 --max-steps 30 --step-duration 10 --initial-workers 12 --eval-episodes 5 --phase-shuffle --phase-shuffle-seed 42
```

Each run automatically evaluates the latest checkpoint every `--checkpoint-every`
episodes (and at the end of training), logging mean Â± std reward/QoS metrics to
`evaluation_metrics.json` alongside the usual `training_metrics.json`.

### **4. Train DQN Agent (lightweight)**

```bash
cd autoscaling_env/rl
python train_dqn.py --episodes 120 --max-steps 30 --step-duration 8 --initial-workers 12 --eval-episodes 3
```

The DQN trainer mirrors SARSA's reporting: timestamped `dqn_run_*` directories with
per-step logs (reward, QoS, **mean/max workers, processed tasks, QoS violations, unfinished tasks**),
checkpoints (`.pt`), `training_metrics.json`, optional `evaluation_metrics.json`, and plots.

> **Tip:** Tune the Polyak target smoothing with `--target-tau` (default `0.01`). Set `1.0` to fall back to the previous hard-copy target network.

### **5. Evaluate Trained Model**

```bash
# Evaluate SARSA
python test_sarsa.py --model runs/sarsa_run_<timestamp>/models/sarsa_final.pkl --initial-workers 12

# Evaluate DQN
python test_dqn.py --model runs/dqn_run_<timestamp>/models/dqn_final.pt --initial-workers 12
```

Both evaluation scripts now emit SARSA-style per-step tables, per-episode plots,
and JSON summaries with the same aggregate metrics as the comparison tooling
(`total_reward`, `mean_reward`, `mean_qos`, `final_qos`, `mean/max workers`, scaling/no-op counts).

---

### **6. Plot Single-Episode Comparison (SARSA, DQN & Baselines)**

```bash
cd autoscaling_env
python -m compare_policies \
  --model rl/runs/sarsa_run_<timestamp>/models/sarsa_final.pkl \
  --dqn-model rl/runs/dqn_run_<timestamp>/models/dqn_final.pt \
  --initial-workers 12 \
  --max-steps 40 \
  --agents sarsa dqn reactiveaverage reactivemaximum
```

Generates a shared-step comparison plot, aggregated statistics, and a log under
`autoscaling_env/runs/comparison/compare_<timestamp>/`. Re-run the plot without
new simulations using `--plot-only --input-dir <existing_run> [--agents ...]`.

> **Re-use baselines:** pass `--baseline-cache <previous_compare_dir>` to reuse
> cached ReactiveAverage / ReactiveMaximum results while re-evaluating SARSA or DQN.

---

## ğŸ¯ Key Features

### **1. Custom OpenFaaS Template**

**Problem**: Standard templates are for short-lived functions  
**Solution**: Custom `python3-http-skeleton` template for long-running functions

**Features**:
- Indefinite execution with `while True` loop and gamma-based image-size sampling
- Flask + Waitress HTTP server
- 12-hour timeouts
- Shared utilities integration

### **2. RL Environment**

**Gym-compatible interface** for training autoscaling policies:

- **Observation**: 9D vector [input_q, worker_q, result_q, output_q, workers, avg_time, max_time, arrival_rate, qos_rate]
- **Action**: 3 discrete actions (-1, 0, +1 workers)
- **Reward**: Configurable blend of QoS delta, normalized queue penalty, worker/idle cost, and scaling penalty (see `utilities/configuration.yml`)
- **Workload**: Task generator draws processing times from a gamma distribution (mean 1.5â€¯s, shape 4.0 by default) and derives image sizes via the calibrated quadratic model
- **Episode Termination**: Task-driven (ends when all tasks complete or max steps reached)

### **3. RL Agent Toolkit**

- **Algorithms**: Tabular SARSA with discretisation + eligibility traces; lightweight DQN with replay buffer, Double DQN target updates, and observation normalisation
- **Training Scripts**: `train_sarsa.py` / `train_dqn.py` share CLI ergonomics, per-step logging tables, checkpointing, evaluation hooks, and plotting outputs
- **Evaluation utilities**: `test_sarsa.py` and `test_dqn.py` manage run directories, emit the same step-by-step tables, and generate per-episode plots. `compare_policies.py` captures single-episode trajectories for SARSA and both reactive baselines on shared axes.

### **4. Baseline Policies**

- **ReactiveAverage**: Conservative, horizon-based planning with average processing time
- **ReactiveMaximum**: Safety-focused, horizon-based planning with max processing time and safety factor

### **5. QoS Tracking**

- Task deadlines based on processing time
- QoS success/failure tracking
- Deadline compliance metrics

### **6. Graceful Scaling**

- **Scale up**: Deploy new workers, invoke immediately
- **Scale down**: SYN/ACK protocol, finish current task
- **Workload shaping**: Emitter reads `phase_definitions` from `utilities/configuration.yml` to produce configurable multi-phase arrivals

---

## ğŸ“Š Performance Comparison

| Method          |  QoS Rate [%] | Mean Workers |  Max Workers | Scaling Actions| No-op Actions|
|-----------------|---------------|--------------|--------------|----------------|--------------|
| **SARSA**       | 99.20 Â± 0.52  | 14.78 Â± 0.49 | 17.80 Â± 0.75 |  24.00 Â± 2.05  | 4.90  Â± 1.92 |
| **DQN**         | 94.91 Â± 3.79  | 14.40 Â± 0.77 | 15.60 Â± 1.02 |   3.60 Â± 1.02  | 24.60 Â± 1.69 |
| ReactiveAverage | 50.87 Â± 4.38  | 10.79 Â± 0.31 | 16.60 Â± 0.66 |  28.40 Â± 0.92  | 1.00  Â± 1.00 |
| ReactiveMaximum | 65.61 Â± 14.26 | 13.92 Â± 0.68 | 18.80 Â± 0.40 |  26.80 Â± 1.25  | 1.70  Â± 0.78 |

---

## ğŸ“ Design Principles

### **1. Separation of Concerns**

- **Emitter**: Task generation
- **Worker**: Task processing (scalable)
- **Collector**: Result aggregation

### **2. Loose Coupling**

- Redis queues for communication
- No direct HTTP calls between functions
- Easy to add/remove workers

### **3. Horizontal Scalability**

- Only workers scale (1-32 instances)
- Emitter and collector remain at 1 instance

### **4. Graceful Degradation**

- Workers poll with timeout (no busy-waiting)
- SYN/ACK protocol for graceful shutdown
- Workers finish current task before exiting

### **5. Observability**

- Real-time queue monitoring
- QoS tracking (deadline compliance)
- Performance metrics (work time, queue lengths)

---

## ğŸ”§ Configuration

Key parameters in `utilities/configuration.yml`:

- **Workload**: `base_rate` (300 tasks/min), `phase_definitions` (list of per-phase multipliers, durations, oscillation controls) consumed by `image_processing/task_generator.py`
- **Scaling**: `min_workers` (1), `max_workers` (32), `initial_workers` (12)
- **Observation window**: `observation_window` (10s), `step_duration` (10s by default across scaling + observation), optional reactive `horizon`
- **Processing time sampling**: `target_mean_processing_time` (default 1.5â€¯s) and `processing_time_shape` (default 4.0) control gamma draws for image sizes in the function utilities
- **Reward**: `reward` block with QoS target, queue target, idle threshold, and per-term weights
- **QoS**: Task deadlines based on calibrated processing time model (DEADLINE_COEFFICIENT = 2.0)
- **SARSA**: `learning_rate` (0.1), `gamma` (0.99), `epsilon_decay` (0.995)
- **DQN**: Replay capacity, batch size, epsilon schedule, target update cadence are configurable via `train_dqn.py`

---

## ğŸ“š Documentation

### **Component READMEs**

- **OpenFaaS Functions**: `openfaas-functions/README.md`
- **Autoscaling Environment**: `autoscaling_env/README.md`
- **Baseline Policies**: `autoscaling_env/baselines/README.md`
- **RL Agents**: `autoscaling_env/rl/README.md`

---

## ğŸ› Troubleshooting

### **Port Forward Issues**

```bash
# Kill existing port forwards
pkill -f "port-forward"

# Restart port forwards
kubectl port-forward -n openfaas svc/gateway 8080:8080 &
kubectl port-forward -n redis svc/redis-master 6379:6379 &
```

### **Function Not Responding**

```bash
# Check function status
faas-cli list
kubectl get pods -n openfaas-fn

# Check logs
kubectl logs -n openfaas-fn -l faas_function=worker-1 --tail=50
```

### **Redis Connection Issues**

```bash
# Test Redis connection
redis-cli ping

# Check queue lengths
redis-cli LLEN worker_queue
```

### **Training Issues**

- **QoS stays low**: Increase observation window, adjust reward weights
- **Continuous scale-up**: Revisit reward queue weight and reactive horizon settings
- **Training too slow**: Reduce steps/episode, reduce step duration

---

## ğŸš€ Next Steps

### **1. Experiment with Workloads**

Modify `configuration.yml` to test different patterns:
- Constant load
- Burst patterns
- Sine wave variations

### **2. Tune Hyperparameters**

Adjust RL agent parameters:
- Learning rate
- Discount factor
- Observation window
- Reward weights

### **3. Extend Functionality**

Add new features:
- Multi-objective optimization
- Cost-aware scaling
- Predictive scaling
- Custom reward functions

### **4. Deploy to Production**

Use trained models for real workloads:
- Load model from checkpoint
- Integrate with monitoring system
- Set up alerting for QoS violations

---

## ğŸ“ Summary

This project provides:

- âœ… **Complete RL-based autoscaling system** for OpenFaaS
- âœ… **Gym-compatible environment** for training
- âœ… **Baseline policies** for comparison
- âœ… **QoS-aware rewards** for deadline compliance
- âœ… **Graceful scaling** with SYN/ACK protocol
- âœ… **Comprehensive documentation** and guides

**Train RL agents to learn optimal autoscaling policies for serverless workflows!** ğŸ‰

---

## ğŸ“„ License

This repository's original code is licensed under the MIT License (see [LICENSE](LICENSE)).

The OpenFaaS code under `openfaas/` is included as Git submodules and remains governed by its own license terms (see `openfaas/faas/LICENSE` and `openfaas/faas-netes/LICENSE` in those submodules).
